<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Vanessa H. Tan | publications</title>
    <meta name="author" content="Vanessa H. Tan" />
    <meta name="description" content="<b>*</b> denotes equal contribution and joint lead authorship" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŠ</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://vtan05.github.io/publications/">
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://vtan05.github.io/"><span class="font-weight-bold">Vanessa</span> H.  Tan</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/cv.pdf">cv</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">publications</h1>
            <p class="post-description"><b>*</b> denotes equal contribution and joint lead authorship</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AEROSPACE</abbr></div>

        <!-- Entry bib key -->
        <div id="mata-rl2" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Sample Efficient Deep Reinforcement Learning for Diwata Microsatellite Reaction Wheel Attitude Control</div>
          <!-- Author -->
          <div class="author">Sarmiento, J-A.,Â 
                  <em>Tan, V.</em>,Â Talampas, M.C.,Â and Naval Jr., P.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Aerospace Systems</em> 2022
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://link.springer.com/article/10.1007/s42401-022-00169-3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The Philippines has launched Diwata satellites to undertake different scientific missions. Low-orbit microsatellites are prone to external disturbances affecting their pointing accuracy; hence, an autonomous attitude control mechanism is vital to its operations. Deep reinforcement learning (DRL) has been proven effective in learning optimal control. There has been prior work regarding using DRL for the satelliteâ€™s reaction wheel attitude control in Mission, Attitude, and Telemetry Analysis (MATA)â€”a simulation environment using Unity for Diwata satellites. However, results show that the applied methods are sample inefficient and still underperform on specific metrics against Diwataâ€™s current attitude control system. In addition, using Unityâ€™s Machine Learning Agents toolkit (ML-Agents) limits the training to Soft-Actor Critic (SAC) and Proximal Policy Optimization (PPO). This study aims to extend the prior research using Twin-Delayed Deep Deterministic Policy Gradient (TD3) and Prioritized Experience Replay (PER) to improve the performance and sample efficiency of the satellite agent. The trainingwas done usingOpenAIGym connected to theMATAsimulation environment.We conclude that TD3-PER outperforms the algorithms of SAC, PPO, and PID of the prior study in both sample efficiency and control performance.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">SMALLSAT</abbr></div>

        <!-- Entry bib key -->
        <div id="mata-rl" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">MATA-RL: Continuous Reaction Wheel Attitude Control Using the MATA Simulation Software and Reinforcement Learning</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.</em>,Â Labrador, J.L.,Â and Talampas, M.C.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Proceedings of the AIAA/USU Conference on Small Satellites, Year in Review - Research &amp; Academia</em> 2021
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://digitalcommons.usu.edu/smallsat/2021/all2021/246/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/libBcmhUO4Q" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As earth observation satellites, Diwata microsatellites need to have a high degree of target pointing accuracy. Additionally, being in low orbit, they could experience strong external disturbances. Current methods for attitude control have proven to be effective. However, they are prone to changes in control and mass parameters. In this paper, we explore using Deep Reinforcement Learning (RL) for attitude control. This paper also leverages on Diwataâ€™s simulator, MATA: Mission, Attitude, and Telemetry Analysis (MATA) software, in training the RL agent. We implemented two RL algorithms: Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC). We then simulated different scenarios and compared the performance of these algorithms to that of Diwataâ€™s current attitude controller, the Proportional-Integral-Derivative (PID) control. Our results show that reinforcement learning can outperform traditional controllers in terms of settling time, overshoot, and stability. The results of this research will help solve problems in conventional attitude controllers and enable satellite engineers to design a better Attitude Determination and Control System (ADCS).</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">SMALLSAT</abbr></div>

        <!-- Entry bib key -->
        <div id="mata-cloud" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">MATA-Cloud: A Cloud Detection and Dynamic Attitude Correction Evaluation Software</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.</em>,Â Banatao, J.A.,Â Labrador, J.L.,Â Mabaquiao, L.C.,Â Fortes, F.F.,Â and Talampas, M.C.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Proceedings of the AIAA/USU Conference on Small Satellites, Flight &amp; Ground Software</em> 2021
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://digitalcommons.usu.edu/smallsat/2021/all2021/103/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/fj8iwdpnfYA" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the increasing demand for high-resolution images from earth observation satellites, there is a need to optimize the usability of the images being downloaded in the ground stations. Most captured satellite images are not usable for certain applications due to high cloud cover percentage. To address this problem, this research demonstrates a cloud detection and dynamic attitude correction evaluation software. This software explores two key experiments. First is evaluating different image processing and machine learning-based approaches to detect cloud cover. The cloud detection algorithms were evaluated based on their accuracy, latency, and memory consumption. The second is exploring dynamic attitude correction to minimize the effect of cloud cover on captured images. Results show that our software can help test algorithms that increase the usability of captured images.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TENCON</abbr></div>

        <!-- Entry bib key -->
        <div id="mata" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">MATA: Mission, Attitude, and Telemetry Analysis Software for Micro-Satellites</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.*</em>,Â Labrador, J.L.*,Â and Talampas, M.C.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE REGION 10 CONFERENCE (TENCON),</em> 2020
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9293937" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/maknppT0bT8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>With the rise in popularity of small satellites, there has been an increasing demand for a software tool that covers different stages of satellite development. In this paper, we extend a small satellite simulation software originally developed for earth-observation satellites Diwata-1 and Diwata-2 to support other satellite missions. This support covers various stages: from ideation, development, and up to post-launch assessment. This paper focuses on the Mission, Attitude, and Telemetry Analysis (MATA) software, which can simulate orbit, attitude, and camera views from planned earth-observation missions. Satellite engineers can also use MATA in a hardware-in-the- loop configuration, serving as one of the last functionality checks before launching the satellite. MATA can also read telemetry files from an orbiting satellite and re-project it in a virtual environment for a more intuitive assessment. This paper also covers the implemented framework for the simulator. This framework would help future developers to extend the simulator to other applications like star tracking simulations, mixed reality satellite training, and space educational software.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TENCON</abbr></div>

        <!-- Entry bib key -->
        <div id="multitask" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Multi-task Learning for Detection, Recovery, and Separation of Polyphonic Music</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.</em>,Â and De Leon, F.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE REGION 10 CONFERENCE (TENCON),</em> 2020
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9293783/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/3Q87nJLslrU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Music separation aims to extract the signals of individual sources from a given audio mixture. Recent studies explored the use of deep learning algorithms for this problem. Although these algorithms have proven to have good performance, they are inefficient as they need to learn an independent model for each sound source. In this study, we demonstrate a multi-task learning system for music separation, detection, and recovery. The proposed system separates polyphonic music into four sound sources using a single model. It also detects the presence of a source in the given mixture. Lastly, it reconstructs the input mixture to help the network further learn the audio representation. Our novel approach exploits the shared information in each task, thus, improving the separation performance of the system. It was determined that the best configuration for the multi-task learning is to separate the sources first, followed by parallel modules for classification and recovery. Quantitative and qualitative results show that the performance of our system is comparable to baselines for separation and classification.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ISMAC</abbr></div>

        <!-- Entry bib key -->
        <div id="ismac" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Time-Frequency Representations for Single-Channel Music Source Separation</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.</em>,Â and Leon, F.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Symposium on Multimedia and Communication Technology (ISMAC),</em> 2019
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9293783/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Inspired by the success of image classification and speech recognition, deep learning algorithms have been explored to solve music source separation. Solving this problem would open to a wide range of applications like automatic transcription, audio post-production, and many more. Most algorithms usually use the Short Time Fourier Transform (STFT) as the Time-Frequency (T-F) input representation. Each deep learning model has a different configuration for STFT. There is no constant STFT parameters that is used in solving music source separation. This paper explores the different parameters for STFT and investigates another representation, the Constant-Q Transform, in separating three individual sound sources. Results of experiments show that dilated convolutional layers are great for STFT while normal convolutional layers are great for CQT. The best T-F representation for music source separation is STFT with dilated CNNs and a soft masking method. Furthermore, researchers should still consider the parameters of the T-F representations to have better performance for their deep learning models.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">WICON</abbr></div>

        <!-- Entry bib key -->
        <div id="wicon" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Audio Event Detection Using Wireless Sensor Networks Based on Deep Learning</div>
          <!-- Author -->
          <div class="author">Mendoza, J.M.*,Â 
                  <em>Tan, V.*</em>,Â Fuentes, V.,Â Perez, G.,Â and Tiglao, N.M.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering,</em> 2019
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://link.springer.com/chapter/10.1007%2F978-3-030-06158-6_11" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Wireless acoustic sensor network is useful for ambient assisted living applications. Its capability of incorporating an audio event detection and classification system helps its users, especially elderly, on their everyday needs. In this paper, we propose using convolutional neural networks (CNN) for classifying audio streams. In contrast to AAL systems using traditional machine learning, our solution is capable of learning and inferring activities in an end-to-end manner. To demonstrate the system, we developed a wireless sensor network composed of Raspberry Pi boards with microphones as nodes. The audio classification system results to an accuracy of 83.79% using a parallel network for the Urban8k dataset, extracting constant-Q transform (CQT) features as system inputs. The overall system is scalabale and flexible in terms of the number of nodes, hence it is applicable on wide areas where assisted living applications are utilized.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TENCON</abbr></div>

        <!-- Entry bib key -->
        <div id="vrex" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Vrex: A Framework for Immersive Virtual Reality Experiences</div>
          <!-- Author -->
          <div class="author">Blonna, R.,Â Tan, M.S.,Â 
                  <em>Tan, V.</em>,Â Mora, A.P.,Â and Atienza, R.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Region Ten Symposium (Tensymp),</em> 2018
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8692018/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/-fD6U2DncZU" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Virtual Reality (VR) is believed to be the future of gaming and even application platforms. However, creating a VR application from scratch takes up a lot of time and research. Virtual Reality frameworks simplify game development by allowing the developer focus on the actual design and system rather than dealing with the core functionalities and interactions of a VR application. In this paper, we present a Virtual Reality framework using Unity3D and the HTC Vive. With this framework, any developer can easily create a VR environment with interactions, scene objectives, player explorations, and many more. This framework is used in the creation of the adventure fantasy game, Eldervine, and adapted for the scene creator application, ANEEME. Results of experiments conducted show the frameworkâ€™s usability in creating different VR applications and its capability to make the interactions intuitive and the experience immersive.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">SIGGRAPH</abbr></div>

        <!-- Entry bib key -->
        <div id="aneeme" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">ANEEME: Synthesizing and Sharing Animation Building Blocks for Rapid Creation of 3D Virtual Scenes</div>
          <!-- Author -->
          <div class="author">
                  <em>Tan, V.</em>,Â Atienza, R.,Â Saludares, M.I.,Â Casimiro, J.,Â and Viola, M.S.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>SIGGRAPH Asia VR Showcase,</em> 2017
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://dl.acm.org/doi/abs/10.1145/3139468.3139479" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
            <a href="https://youtu.be/ZC19KRPFkVw" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>ANEEME focuses on building technologies that rapidly synthesize animated visual scenes. This virtual reality experience immerses users to different styles and culture of houses around the world. There are two modes in this application: build mode and play mode. In build mode, users can integrate 3D models from the local computer or from ANEEMEâ€™s online repository into the virtual environment. Using ANEEMEâ€™s automatic skeletal rigging, users can easily incorporate and animate humanoid objects such as toys, figurines, and even their own avatar. These enable users to design, build and customize their dream house. During play mode, users can interact with the objects inside the environment. They can watch videos, listen to music, play with different instruments, and many more. Photo and video capture capabilities available in this mode also allow users to easily share their virtual environment through their social media accounts.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICCSCE</abbr></div>

        <!-- Entry bib key -->
        <div id="bs" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Study of Automatic Melody Extraction Methods for Philippine Indigenous Music</div>
          <!-- Author -->
          <div class="author">Disuanco, J.*,Â 
                  <em>Tan, V.*</em>,Â and Leon, F.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE International Conference on Control System, Computing and Engineering (ICCSCE),</em> 2015
          </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/7482230" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this study, we compared two methods for extracting the melody pitch from select Philippine indigenous music. Pitch is expressed as the fundamental frequency of the main melodic voice or lead instrument of a music sample. Our implementation of automatic melody extraction involves blind source separation and pitch detection. For blind source separation, we implemented the Harmonic-Percussive Source Separation (HPSS) algorithm and the Shifted Non-negative Matrix Factorization (SNMF) algorithm. The HPSS algorithm identifies the harmonic component from the prominent peaks in the spectrogram of a signal while the SNMF algorithm use timbre as criterion. The harmonic component is used to estimate the melody pitch. The HPSS and SNMF source separation algorithms are complemented with salience-based and data driven pitch detection algorithms, respectively. The two systems are evaluated using ten samples of Philippine indigenous music. After source separation, the estimated harmonic and percussive tracks were evaluated through subjective listening tests. Results from subjective tests show that SNMF perform better than HPSS for harmonic and percussive source separation. Moreover, objective tests using standard metrics indicate that the salience-based approach has higher accuracy in identifying the melody than the data driven approach.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2022 Vanessa H. Tan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

